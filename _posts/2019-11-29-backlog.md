---
layout: post
title:  "Backlog of articles to read"
author: gerald
categories: [ ml, rpa ]
image: assets/images/landscape7.jpg
---

Backlog of articles to process with rough categorization
---

**Deep-learning general**
* MIT lecture on Deep Learning [State of the Art](https://www.youtube.com/watch?v=53YvP6gdD7U&feature=youtu.be&t=335)

**Tensorflow general**
* Tensorflow at [O'Reilly conference](https://www.youtube.com/watch?v=MunFeX-0MD8&list=PLQY2H8rRoyvxcmHHRftsuiO1GyinVAwUg)



**NLP - Basics**

* *Embeddings*: 
First step often is turning input words into vectors using embeddings (see. e.g. [here](https://machinelearningmastery.com/what-are-word-embeddings/) for an explanation of word embeddings). These vectos capture some semantics (king-man+woman=queen). These vectors are somewhere in the 200-300 dimensions range. Embedding explanation with Word2Vec focus on [medium](https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca)

* *Attention* [overview](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) and original paper ["Attention is all you need"](https://arxiv.org/pdf/1706.03762.pdf). Basic premise is that enabling the neural network to pay attention to a specific subset of hidden states leads to better results.
* Basics - *Transformer* [overview](http://jalammar.github.io/illustrated-transformer/). Originally intorduced in ["Attention is all you need"](https://arxiv.org/pdf/1706.03762.pdf). Fully annotated code to the paper from Harvard NLP group is [here](http://nlp.seas.harvard.edu/2018/04/03/attention.html). And [Tensorflow library tensor2tensor](https://github.com/tensorflow/tensor2tensor). Interactive tensor2tensor on [Google Colab](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)
* Introductory article for [Hugging Face](https://blog.tensorflow.org/2019/11/hugging-face-state-of-art-natural.html?m=1) NLP library 
**Implementation**
* Getting a big model into [production](https://medium.com/huggingface/scaling-a-massive-state-of-the-art-deep-learning-model-in-production-8277c5652d5f)

**Self improvement / learning**
* Learning course for [GAN](https://developers.google.com/machine-learning/gan/) from Google